{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, csv\n",
    "from urllib import request, error\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import random\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import tempfile\n",
    "import pickle\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "model_dir = '../models'\n",
    "\n",
    "color_mode = 'rgb'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "height = 100\n",
    "width = 100\n",
    "depth = 3 if color_mode == 'rgb' else 1\n",
    "\n",
    "n_layers_to_tune = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'landmark-data-12345'\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'data/sample_submission.csv'\n",
    "dest = '../data/sample_submission.csv'\n",
    "\n",
    "if not os.path.isfile(dest):\n",
    "    bucket.download_file(source, dest)\n",
    "\n",
    "sample_sub = pd.read_csv(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from files\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "filekeys = []\n",
    "\n",
    "if os.path.isfile('pickles/test_filekeys'):\n",
    "    print('loading data from files')\n",
    "    \n",
    "    file = open('pickles/test_filekeys', 'rb')\n",
    "    filekeys = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "else:\n",
    "    print('data pickles dont exist, generating')\n",
    "    objects = bucket.objects.filter(Prefix=\"data/test/\")\n",
    "    for o in objects:\n",
    "        if o.key.endswith('.jpg'):\n",
    "            filekeys.append(o.key)\n",
    "\n",
    "    file = open('pickles/test_filekeys', 'wb')\n",
    "    pickle.dump(filekeys, file)\n",
    "    file.close()\n",
    "\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 113980 test images\n"
     ]
    }
   ],
   "source": [
    "num_filekeys = len(filekeys)\n",
    "print('there are', num_filekeys, 'test images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_s3_file(filekey):\n",
    "    object = bucket.Object(filekey)\n",
    "    tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    with open(tmp.name, 'wb') as f:\n",
    "        object.download_fileobj(f)\n",
    "        img = mpimg.imread(tmp.name)\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img):\n",
    "    try:\n",
    "        processed_img = np.divide(img, 255.0)\n",
    "        processed_img = resize(img, (height, width, depth), mode='reflect', anti_aliasing=True)\n",
    "        return processed_img\n",
    "    except:\n",
    "        return np.zeros((height, width, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "pre_model = applications.VGG19(weights=\"imagenet\", \n",
    "                           include_top=False, \n",
    "                           input_shape=(width, height, 3 if color_mode == 'rgb' else 1))\n",
    "\n",
    "model = Sequential()\n",
    "for idx, layer in enumerate(pre_model._layers):\n",
    "    if idx < len(pre_model._layers) - n_layers_to_tune:\n",
    "        layer.trainable = False\n",
    "    model.add(layer)\n",
    "    \n",
    "model.add(Flatten(input_shape=pre_model.output_shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(14940, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1994)              2043850   \n",
      "=================================================================\n",
      "Total params: 27,837,450\n",
      "Trainable params: 7,813,066\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download weights\n",
    "num_batch_to_load = 4\n",
    "source = 'models/landmark_model_weights' + str(num_batch_to_load) + '.h5'\n",
    "dest = '../models/landmark_model_weights' + str(num_batch_to_load) + '.h5'\n",
    "\n",
    "if not os.path.isfile(dest):\n",
    "    bucket.download_file(source, dest)\n",
    "\n",
    "model.load_weights(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pruned classes to turn pruned prediction into normal prediction\n",
    "file = open('pickles/pruned_classes', 'rb')\n",
    "pruned_classes = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing img 0 after 0.0002486705780029297 seconds\n",
      "processing img 121 after 50.73480296134949 seconds\n",
      "processing img 245 after 101.51376700401306 seconds\n",
      "processing img 373 after 148.15084385871887 seconds\n",
      "processing img 497 after 198.0821497440338 seconds\n",
      "processing img 624 after 247.4419093132019 seconds\n",
      "processing img 749 after 297.3329689502716 seconds\n",
      "processing img 873 after 346.7143979072571 seconds\n",
      "processing img 999 after 393.9162976741791 seconds\n",
      "processing img 1123 after 447.58734107017517 seconds\n",
      "processing img 1246 after 493.74377369880676 seconds\n",
      "processing img 1371 after 540.2479226589203 seconds\n",
      "processing img 1495 after 586.9122774600983 seconds\n",
      "processing img 1618 after 635.558272600174 seconds\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "preds = []\n",
    "idx = 0\n",
    "sub_idx = 0\n",
    "\n",
    "start = time.time()\n",
    "while idx < num_filekeys:\n",
    "    print('processing img',idx,'after',time.time()-start,'seconds')\n",
    "\n",
    "    pred_batch = []\n",
    "    batch_ids = []\n",
    "    while len(pred_batch) < batch_size*4:\n",
    "        if idx < num_filekeys:\n",
    "\n",
    "            filekey = filekeys[idx]\n",
    "            img_id = filekey.split('/')[2].split('.')[0] # get filename, minus jpg\n",
    "#             print('img id is',img_id)\n",
    "            curr_sub_id = sample_sub.iloc[sub_idx]['id']\n",
    "            while curr_sub_id != img_id:\n",
    "                print('but sub id is',curr_sub_id)\n",
    "                batch_ids.append(curr_sub_id)\n",
    "                pred_batch.append(np.zeros((height, width, depth)))\n",
    "                sub_idx += 1\n",
    "                curr_sub_id = sample_sub.iloc[sub_idx]['id']\n",
    "            \n",
    "            batch_ids.append(img_id)\n",
    "\n",
    "            img = load_s3_file(filekey)\n",
    "            processed_img = process_img(img)\n",
    "            pred_batch.append(processed_img)\n",
    "            idx += 1\n",
    "            sub_idx += 1\n",
    "\n",
    "    predictions = model.predict_on_batch(np.array(pred_batch))\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        pred_pruned_class = np.argmax(prediction)\n",
    "        pred_class = pruned_classes[pred_pruned_class]\n",
    "        confidence = np.max(prediction)\n",
    "        preds.append((batch_ids[i], pred_class, confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preds), 'preds') # should be 117703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df = pd.DataFrame()\n",
    "df['id'] = pd.Series([ pred[0] for pred in preds ])\n",
    "df['landmarks'] = pd.Series([ (str(pred[1]) + ' ' + str(pred[2])) for pred in preds ])\n",
    "df.to_csv('pickles/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
