{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os, csv\n",
    "from urllib import request, error\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow 1.10.1\n",
      "Using keras 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print('Using tensorflow', tf.__version__)\n",
    "print('Using keras', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 cpu cores available\n"
     ]
    }
   ],
   "source": [
    "# set constants\n",
    "model_dir = '../models'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "height = 100\n",
    "width = 100\n",
    "color_mode = 'rgb'\n",
    "depth = 3 if color_mode == 'rgb' else 1\n",
    "\n",
    "n_layers_to_tune = 0\n",
    "\n",
    "should_prune = False\n",
    "should_subset = True\n",
    "subset_length = 10000\n",
    "should_debug = False\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_workers = n_cpus - 1 # None defaults to n_cpus\n",
    "print('There are', n_cpus, 'cpu cores available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(*string):\n",
    "    if should_debug:\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'landmark-data-12345'\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from files\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "class_dist = {}\n",
    "classes = []\n",
    "filekeys = []\n",
    "\n",
    "if os.path.isfile('pickles/filekeys'):\n",
    "    print('loading data from files')\n",
    "    \n",
    "    file = open('pickles/filekeys', 'rb')\n",
    "    filekeys = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    file = open('pickles/classes', 'rb')\n",
    "    classes = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    file = open('pickles/class_dist', 'rb')\n",
    "    class_dist = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "else:\n",
    "    print('data pickles dont exist, generating')\n",
    "    objects = bucket.objects.filter(Prefix=\"data/train/\")\n",
    "    for o in objects:\n",
    "        if o.key.endswith('.jpg'):\n",
    "            filekeys.append(o.key)\n",
    "            cl = o.key.split('/')[2]\n",
    "            if cl not in classes:\n",
    "                class_dist[cl] = 1\n",
    "                classes.append(cl)\n",
    "            else:\n",
    "                class_dist[cl] += 1\n",
    "\n",
    "    shuffle(filekeys)\n",
    "    file = open('pickles/filekeys', 'wb')\n",
    "    pickle.dump(filekeys, file)\n",
    "    file.close()\n",
    "\n",
    "    file = open('pickles/classes', 'wb')\n",
    "    pickle.dump(classes, file)\n",
    "    file.close()\n",
    "\n",
    "    file = open('pickles/class_dist', 'wb')\n",
    "    pickle.dump(class_dist, file)\n",
    "    file.close()\n",
    "    \n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 10000 images across 14940 classes\n"
     ]
    }
   ],
   "source": [
    "if should_subset:\n",
    "    filekeys = filekeys[:subset_length]\n",
    "num_filekeys = len(filekeys)\n",
    "num_classes = len(classes)\n",
    "print('there are', num_filekeys, 'images across', num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_s3_file_indirect(filekey):\n",
    "    debug('LS3: loading')\n",
    "    object = bucket.Object(filekey)\n",
    "    debug('LS3: a')\n",
    "    tmp = tempfile.NamedTemporaryFile()\n",
    "    debug('LS3: b')\n",
    "    with open(tmp.name, 'wb') as f:\n",
    "        debug('LS3: c')\n",
    "        object.download_fileobj(f)\n",
    "        debug('LS3: d')\n",
    "        img = mpimg.imread(tmp.name)\n",
    "        debug('LS3: done loading')\n",
    "        return img\n",
    "    \n",
    "def load_s3_file_direct(filekey):\n",
    "    debug('LS3: loading')\n",
    "    object = bucket.Object(filekey)\n",
    "    debug('LS3: a')\n",
    "    file_stream = io.StringIO()\n",
    "    debug('LS3: b')\n",
    "    object.download_fileobj(file_stream)\n",
    "    debug('LS3: c')\n",
    "    img = mpimg.imread(file_stream)\n",
    "    debug('LS3: done loading')\n",
    "    return img\n",
    "\n",
    "def load_s3_file(filekey):\n",
    "    return load_s3_file_indirect(filekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_batch():\n",
    "#     idx = 0\n",
    "#     while True:\n",
    "#         images = []\n",
    "#         labels = []\n",
    "#         while len(images) < batch_size:\n",
    "#             filekey = filekeys[idx]\n",
    "#             label = filekey.split('/')[2]\n",
    "            \n",
    "#             try:\n",
    "#                 img = load_s3_file(filekey)\n",
    "#                 images.append(img)\n",
    "#                 labels.append(label)\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             idx = (idx + 1) % len(filekeys)\n",
    "            \n",
    "#         result = ( np.array(images), np.array(labels) )\n",
    "#         yield result\n",
    "\n",
    "def get_batch():\n",
    "    idx = 0\n",
    "    while True:\n",
    "        images = []\n",
    "        labels = []\n",
    "        while len(images) < 1:\n",
    "            filekey = filekeys[idx]\n",
    "            label = filekey.split('/')[2]\n",
    "            \n",
    "            try:\n",
    "                img = load_s3_file(filekey)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            idx = (idx + 1) % len(filekeys)\n",
    "            \n",
    "        result = ( np.array(images), np.array(labels) )\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfdata_generator():\n",
    "    '''Construct a data generator using tf.Dataset'''\n",
    "\n",
    "    def preprocess_fn(image, label):\n",
    "        '''A transformation function to preprocess raw data\n",
    "        into trainable input. '''\n",
    "        x = tf.image.resize_image_with_pad(tf.cast(image, tf.float32), height, width)\n",
    "        y = tf.one_hot(tf.cast(label, tf.int32), num_classes)\n",
    "        return x, label\n",
    "\n",
    "    dataset = tf.data.Dataset().batch(batch_size).from_generator(get_batch,\n",
    "                output_types=(tf.float32, tf.float32), \n",
    "#                 output_shapes=((width, height, depth), (num_classes,))\n",
    "                )\n",
    "\n",
    "    # Transform and batch data at the same time\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "        preprocess_fn, batch_size,\n",
    "        num_parallel_batches=n_workers))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = tfdata_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "pre_model = applications.VGG19(weights=\"imagenet\", \n",
    "                           include_top=False, \n",
    "                           input_shape=(width, height, depth))\n",
    "\n",
    "model = Sequential()\n",
    "for idx, layer in enumerate(pre_model._layers):\n",
    "    if idx < len(pre_model._layers) - n_layers_to_tune:\n",
    "        layer.trainable = False\n",
    "    model.add(layer)\n",
    "    \n",
    "model.add(Flatten(input_shape=pre_model.output_shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14940)             7664220   \n",
      "=================================================================\n",
      "Total params: 30,311,068\n",
      "Trainable params: 10,286,684\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tell model to save after each epoch\n",
    "class SaveEachEpoch(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        debug('SEE: saving model for epoch', epoch)\n",
    "        filename1 = '/landmark_model_' + str(epoch) + '.h5'\n",
    "        filename2 = '/landmark_model_weights_' + str(epoch) + '.h5'\n",
    "        source1 = model_dir+filename1\n",
    "        dest1 = 'models'+filename1\n",
    "        source2 = model_dir+filename2\n",
    "        dest2 = 'models'+filename2\n",
    "\n",
    "        try:\n",
    "            self.model.save(source1)\n",
    "            bucket.upload_file(source1, dest1)\n",
    "\n",
    "            self.model.save_weights(source2)\n",
    "            bucket.upload_file(source2, dest2)\n",
    "\n",
    "            # todo: clean up/delete model files\n",
    "        except:\n",
    "            debug('SEE: error saving model')\n",
    "            return\n",
    "        \n",
    "        debug('SEE: done saving model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).\n",
      "  0/312 [..............................] - ETA: 0sEpoch 2/5\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).\n",
      "  0/312 [..............................] - ETA: 0sEpoch 3/5\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).\n",
      "  0/312 [..............................] - ETA: 0sEpoch 4/5\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).\n",
      "  0/312 [..............................] - ETA: 0sEpoch 5/5\n",
      "WARNING:tensorflow:Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).Your dataset iterator ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches).\n",
      "  0/312 [..............................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad54091588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_dataset.make_one_shot_iterator(),\n",
    "            epochs=5, \n",
    "            steps_per_epoch=len(filekeys)//batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=[SaveEachEpoch()]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
