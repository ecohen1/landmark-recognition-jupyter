{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_exec = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tf version 1.10.1 with log level 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if eager_exec:\n",
    "    tf.enable_eager_execution()\n",
    "    \n",
    "import os\n",
    "loglevel = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = loglevel\n",
    "\n",
    "print('using tf version', tf.__version__, 'with log level', loglevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os, csv\n",
    "from urllib import request, error\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import boto3\n",
    "import random\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "import tempfile\n",
    "import pickle\n",
    "import math\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.data import TFRecordDataset\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow 1.10.1\n",
      "Using keras 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print('Using tensorflow', tf.__version__)\n",
    "print('Using keras', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 cpu cores available\n"
     ]
    }
   ],
   "source": [
    "# set constants\n",
    "model_dir = '../models'\n",
    "tfrecord_dir = 'tf_data'\n",
    "\n",
    "batch_size = 64\n",
    "tfrecord_batch_size = 500\n",
    "num_classes = 14950\n",
    "\n",
    "height = 100\n",
    "width = 100\n",
    "color_mode = 'rgb'\n",
    "depth = 3 if color_mode == 'rgb' else 1\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_workers = n_cpus - 1 # None defaults to n_cpus\n",
    "print('There are', n_cpus, 'cpu cores available')\n",
    "\n",
    "save_model_weights = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'landmark-data-12345'\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2638 tfrecord files\n",
      "for example: s3://landmark-data-12345/tf_data/1.tfrecord\n"
     ]
    }
   ],
   "source": [
    "prefix = 's3://landmark-data-12345/'\n",
    "filenames = [ prefix + obj.key for obj in bucket.objects.filter(Prefix=tfrecord_dir).all() if obj.key.endswith('tfrecord') ]\n",
    "\n",
    "# filenames = filenames[:1]\n",
    "print('there are', len(filenames), 'tfrecord files')\n",
    "print('for example:', filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "pre_model = applications.VGG19(weights=\"imagenet\", \n",
    "                           include_top=False, \n",
    "                           input_shape=(height, width, depth))\n",
    "\n",
    "model = Sequential()\n",
    "for idx, layer in enumerate(pre_model._layers):\n",
    "    layer.trainable = False\n",
    "    model.add(layer)\n",
    "    \n",
    "model.add(Flatten(input_shape=pre_model.output_shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14950)             7669350   \n",
      "=================================================================\n",
      "Total params: 30,316,198\n",
      "Trainable params: 10,291,814\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "if not eager_exec:\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy', 'categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tell model to save after each epoch\n",
    "class SaveEachEpoch(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if save_model_weights:\n",
    "            epoch = epoch + 1\n",
    "            print('SEE: saving model for epoch', epoch)\n",
    "\n",
    "            filename1 = '/small_landmark_model_' + str(epoch) + '.h5'\n",
    "            filename2 = '/small_landmark_model_weights_' + str(epoch) + '.h5'\n",
    "\n",
    "            source1 = model_dir+filename1\n",
    "            dest1 = 'models'+filename1\n",
    "            source2 = model_dir+filename2\n",
    "            dest2 = 'models'+filename2\n",
    "\n",
    "            try:\n",
    "                self.model.save(source1)\n",
    "                bucket.upload_file(source1, dest1)\n",
    "\n",
    "                self.model.save_weights(source2)\n",
    "                bucket.upload_file(source2, dest2)\n",
    "\n",
    "            except:\n",
    "                print('SEE: error saving model')\n",
    "                return\n",
    "\n",
    "            print('SEE: done saving model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy arrays dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batches = []\n",
    "pred_batch = []\n",
    "labels = []\n",
    "num_records = 0\n",
    "\n",
    "# for idx, filename in enumerate(filenames):\n",
    "#     print(idx+1, 'of', len(filenames), 'tfrecords')\n",
    "#     record_iterator = tf.python_io.tf_record_iterator(path=filename)\n",
    "    \n",
    "#     for idx2, record_string in enumerate(record_iterator):\n",
    "#         num_records += 1\n",
    "#         if len(pred_batch) == batch_size:\n",
    "#             pred_batches.append(pred_batch)\n",
    "#             pred_batch = []\n",
    "#             print('finished batch', len(pred_batches))\n",
    "\n",
    "#         example = tf.train.Example()\n",
    "#         example.ParseFromString(record_string)\n",
    "\n",
    "#         img_byte_string = example.features.feature['image'].bytes_list.value[0]\n",
    "#         shape = example.features.feature['shape'].int64_list.value    \n",
    "#         if len(shape) != 3:\n",
    "#             print('shape is', shape, ', skipping')\n",
    "#             continue\n",
    "\n",
    "#         tempBuff = io.BytesIO()\n",
    "#         tempBuff.write(img_byte_string)\n",
    "#         tempBuff.seek(0)\n",
    "\n",
    "#         image_data = Image.open(tempBuff).convert(\"RGB\")\n",
    "#         image_data = np.array(image_data).reshape(shape)\n",
    "#         image_data = resize(image_data, (height, width, depth), anti_aliasing=True)\n",
    "        \n",
    "#         pred_batch.append(image_data)\n",
    "        \n",
    "#         label = example.features.feature['label'].int64_list.value[0]\n",
    "#         labels.append(label)\n",
    "        \n",
    "# if len(pred_batch) > 0:\n",
    "#     pred_batches.append(pred_batch)\n",
    "#     pred_batch = []\n",
    "    \n",
    "# plt.figure()\n",
    "# imshow(pred_batches[0][0])\n",
    "\n",
    "# print('processed', num_records, 'records')\n",
    "# print(len(pred_batches), 'batches of size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for pred_batch in pred_batches:\n",
    "    for image_data in pred_batch:\n",
    "        x.append(image_data)\n",
    "        \n",
    "y = []\n",
    "for label in labels:\n",
    "    one_hot = np.zeros((num_classes))\n",
    "    one_hot[label] = 1 # starts at 0\n",
    "    y.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(x).shape)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(y).shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(tensor):\n",
    "    features = {\n",
    "        'shape': tf.FixedLenFeature([], dtype=tf.int64)\n",
    "    }\n",
    "    example = tf.parse_single_example(tensor, features)\n",
    "\n",
    "    return tf.not_equal(tf.size(example['shape']), 0)\n",
    "\n",
    "def custom_decode_image(tensor):\n",
    "    return tf.cast(tf.image.decode_jpeg(tensor, channels=depth), tf.float32)\n",
    "\n",
    "def x_preprocess_fn(tensor):\n",
    "    features = {\n",
    "        'label': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "        'image': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    }\n",
    "    example = tf.parse_single_example(tensor, features)\n",
    "\n",
    "    x = custom_decode_image(example['image'])\n",
    "    \n",
    "    x = tf.multiply(x, tf.constant(1.0 / 255, dtype=tf.float32))\n",
    "\n",
    "    return x\n",
    "\n",
    "def resize_fn(tensor):\n",
    "    x = tf.image.resize_bilinear(tensor, (height, width), align_corners=False)\n",
    "#     x = tf.image.resize_nearest_neighbor(tensor, (height, width), align_corners=False)\n",
    "#     x = tf.image.resize_area(tensor, (height, width), align_corners=False)\n",
    "#     x = tf.image.resize_image_with_pad(tensor, height, width)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def y_preprocess_fn(tensor):\n",
    "    features = {\n",
    "        'label': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "        'image': tf.FixedLenFeature([], dtype=tf.string),\n",
    "    }\n",
    "    example = tf.parse_single_example(tensor, features)\n",
    "\n",
    "    y = tf.one_hot(tf.cast(example['label'], tf.int32), num_classes)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "predataset = TFRecordDataset(filenames).filter(filter_func)\n",
    "\n",
    "xdataset = predataset.map(x_preprocess_fn).batch(1).map(resize_fn).flat_map(lambda x: tf.data.TFRecordDataset.from_tensor_slices(x)).batch(batch_size)\n",
    "\n",
    "ydataset = predataset.map(y_preprocess_fn).batch(batch_size)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset.zip((xdataset, ydataset)).repeat(10)\n",
    "\n",
    "# if eager_exec:\n",
    "#     for y in ydataset.make_one_shot_iterator():\n",
    "#         print(np.argmax(y.numpy()))\n",
    "\n",
    "# if eager_exec:\n",
    "#     for x,y in dataset.take(10).make_one_shot_iterator():\n",
    "#         print(x.numpy()[0])\n",
    "#         plt.figure()\n",
    "#         imshow(x.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 10 epochs with batch size 1 for 1319000 steps per epoch\n",
      "Epoch 1/10\n",
      "    906/1319000 [..............................] - ETA: 40:15:27 - loss: 8.8175 - acc: 0.0430 - categorical_accuracy: 0.0430"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e7dcdc9651c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSaveEachEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m          )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "n_epochs = 10\n",
    "\n",
    "# print('training on', len(x), 'examples for', n_epochs, 'epochs with batch size', batch_size)\n",
    "# model.fit(x=np.array(x),\n",
    "#           y=np.array(y),\n",
    "#           epochs=n_epochs,\n",
    "#           batch_size=batch_size,\n",
    "#           verbose=1,\n",
    "#           callbacks=[SaveEachEpoch()]\n",
    "#          )\n",
    "\n",
    "num_examples = len(filenames) * tfrecord_batch_size\n",
    "steps_per_epoch = num_examples // batch_size\n",
    "if num_examples % batch_size > 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "print('training for', n_epochs, 'epochs with batch size', batch_size, 'for', steps_per_epoch, 'steps per epoch')\n",
    "model.fit(dataset.make_one_shot_iterator(),\n",
    "          epochs=n_epochs,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          callbacks=[SaveEachEpoch()],\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_preds = model.predict(np.array(x))\n",
    "preds = []\n",
    "for i, pred in enumerate(softmax_preds):\n",
    "    pred_class = np.argmax(pred)\n",
    "    confidence = np.max(pred)\n",
    "    preds.append(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6323232323232323\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "for idx, pred in enumerate(preds):\n",
    "    if pred == labels[idx]:\n",
    "        n_correct += 1\n",
    "        \n",
    "print('accuracy:', float(n_correct)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6051, 1281, 1157, 1904, 14912, 14669, 7505, 9633, 10067, 8495, 1553, 9779, 2622, 4645, 2311, 3301, 9193, 9201, 8598, 9633, 3550, 14931, 3804, 9633, 4827, 623, 14565, 9633, 6599, 9738, 5090, 9633, 9633, 4981, 5554, 11010, 4735, 6051, 6051, 11425, 9633, 9633, 9633, 8429, 9633, 9633, 9633, 2963, 12220, 6599, 12142, 9633, 12533, 9633, 9633, 9633, 6642, 1828, 1946, 3504, 2665, 6051, 9633, 14873, 8143, 12829, 5260, 9633, 2743, 9633, 9633, 2996, 3034, 10932, 5574, 9633, 13208, 5661, 13773, 10567, 10834, 8090, 5376, 9633, 6599, 9335, 9633, 7700, 8169, 14565, 4981, 9633, 9633, 6091, 7761, 8274, 8487, 10932, 9633, 823, 6051, 5946, 9633, 9633, 6599, 6091, 5635, 11058, 6051, 9633, 6051, 6051, 10075, 12609, 11424, 1376, 2870, 3550, 4827, 9633, 9633, 10757, 2885, 8705, 7150, 7942, 7218, 9779, 428, 9633, 3238, 7083, 369, 9633, 2743, 6599, 960, 9633, 11864, 9334, 2429, 11475, 5506, 5376, 5864, 2209, 9633, 2743, 9633, 5554, 14378, 5380, 4645, 9633, 5376, 2842, 12718, 9779, 8274, 7262, 12866, 5794, 9633, 1647, 2743, 9999, 9633, 8773, 9617, 9633, 10045, 12507, 878, 10184, 14619, 9633, 12498, 649, 12533, 5376, 9739, 9818, 13489, 1770, 3878, 4645, 5313, 2710, 6051, 9633, 5265, 5762, 9633, 2676, 187, 10067, 2205, 9633, 4981, 1746, 5074, 2743, 11720, 6091, 11351, 9633, 960, 4722, 9633, 9633, 9633, 6051, 2429, 428, 12220, 3314, 6051, 6051, 5063, 3574, 7674, 960, 9633, 6051, 5661, 2743, 7840, 4981, 12966, 547, 11254, 4981, 7256, 8029, 2061, 9633, 13776, 9633, 9633, 6796, 14565, 1124, 6051, 11714, 9633, 10644, 9434, 1745, 4300, 14912, 12360, 10067, 9633, 11153, 3924, 14669, 7559, 4723, 9633, 10235, 6051, 6051, 10932, 9633, 2743, 9119, 1677, 2610, 14085, 9779, 1356, 7505, 5955, 12090, 703, 1472, 9633, 2996, 5554, 6685, 428, 7262, 9633, 6051, 9131, 1746, 10184, 7661, 1553, 9633, 10184, 10496, 8135, 5260, 6051, 10496, 187, 9633, 9633, 9633, 14914, 7000, 5371, 10858, 10522, 9633, 9633, 4981, 6599, 2080, 3787, 4551, 8763, 2802, 2200, 5574, 5931, 9633, 9633, 11145, 428, 14359, 14090, 4981, 3426, 1211, 9633, 8964, 9633, 8529, 6977, 12220, 7505, 9633, 2774, 5554, 11780, 6051, 14523, 6051, 2743, 6051, 9633, 9633, 14565, 9633, 13526, 4156, 11864, 9633, 7999, 111, 10675, 13594, 5260, 428, 7357, 5376, 9633, 6699, 12776, 6599, 5554, 9033, 9739, 2743, 2743, 4723, 9779, 9633, 5794, 9642, 9617, 12981, 9633, 2743, 10522, 9633, 428, 4735, 6051, 14565, 5661, 6249, 5101, 7661, 5376, 9119, 9633, 9633, 11753, 2281, 1623, 9633, 9633, 7661, 9633, 3908, 9633, 9633, 9633, 13209, 7637, 12222, 9633, 8495, 1553, 2560, 10045, 1031, 4981, 5661, 9841, 9633, 11139, 9633, 2209, 8536, 428, 9135, 5887, 8161, 9633, 8429, 9779, 10932, 6367, 9633, 9633, 12728, 14565, 9633, 4987, 9660, 9835, 10624, 12609, 9633, 9633, 13715, 10184, 2743, 1046, 3114, 8614, 9633, 6599, 12360, 14111, 9249, 3924, 8054, 8094, 8302, 4697, 9633, 9633, 1878, 2200, 10075, 9633, 5946, 6455, 9633, 6599, 13124, 6271, 6971, 8274, 1296, 9633, 9439, 9633, 10197, 13710, 9296, 9633, 5726, 2996, 6599, 6450, 9633, 14914, 428, 10067, 7041, 6051, 9633, 6051, 9633, 6599, 11679, 6051, 2330, 9086]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6103, 1281, 1157, 1904, 3495, 1383, 1472, 3283, 14860, 6599, 10378, 9779, 2622, 4645, 2311, 3301, 9193, 9201, 8598, 9633, 6696, 14931, 3804, 8063, 4827, 623, 12172, 9633, 3695, 9738, 5090, 6696, 2061, 10653, 1140, 11010, 4735, 9638, 5506, 11425, 9633, 4352, 5714, 5880, 3137, 6846, 9633, 2963, 12220, 6347, 12142, 10368, 12533, 3924, 12220, 12992, 6642, 1828, 1946, 3504, 2665, 5259, 11536, 14873, 8143, 12829, 12514, 4522, 2743, 5554, 9633, 3736, 3034, 10932, 11643, 7096, 13208, 9479, 13773, 10567, 10834, 8090, 5376, 2339, 6231, 7013, 6008, 7700, 8169, 11037, 11815, 14562, 14104, 9779, 7761, 4352, 8487, 10932, 9633, 823, 6051, 2044, 11755, 13873, 6599, 9779, 5635, 11058, 6051, 6696, 6651, 1213, 13926, 12950, 11424, 1376, 2870, 3550, 3609, 12676, 4085, 10757, 2885, 8705, 7150, 7942, 7218, 7476, 14386, 428, 3238, 7083, 369, 9633, 2743, 9571, 14079, 14544, 9335, 9334, 6696, 11475, 5506, 5376, 5864, 2209, 8501, 7730, 9633, 5554, 14378, 5380, 7922, 8063, 3518, 2842, 12718, 9779, 6112, 7262, 12866, 4352, 3065, 1647, 14703, 9999, 6274, 8773, 9617, 4523, 10045, 12507, 878, 10184, 14619, 11394, 12498, 649, 172, 5376, 9144, 9818, 13489, 1770, 3878, 10496, 5313, 2710, 6051, 5955, 5265, 5762, 5203, 2676, 11580, 10067, 2205, 9633, 4981, 8163, 5074, 2743, 11720, 6091, 11351, 9633, 3725, 4722, 6696, 9633, 8429, 7680, 2429, 6651, 12220, 3314, 9633, 6051, 5063, 3574, 7674, 960, 6051, 11430, 476, 2743, 7840, 13332, 12966, 547, 11254, 1188, 7256, 8029, 2061, 10900, 13776, 10724, 2241, 6796, 14565, 1124, 6051, 11714, 10577, 10644, 9434, 1745, 4300, 14912, 12360, 10067, 9633, 11153, 13410, 14669, 7559, 4723, 2131, 10235, 6051, 3187, 452, 6599, 2061, 10192, 1677, 2610, 14085, 9779, 1356, 4108, 5955, 12090, 703, 1472, 4498, 7445, 5554, 6685, 4987, 10465, 11812, 6599, 9131, 1746, 10184, 7661, 1553, 2729, 10184, 8488, 8135, 5260, 2338, 7390, 187, 11856, 6599, 476, 14914, 7000, 5371, 10858, 7661, 13526, 7721, 4981, 11411, 2080, 3787, 4551, 8763, 2802, 4981, 5574, 5931, 8120, 9633, 11145, 428, 14359, 14090, 13492, 3426, 1211, 4875, 8964, 2131, 8529, 6977, 12220, 7505, 13447, 2774, 5554, 11780, 2920, 14523, 6051, 3283, 6051, 3065, 13696, 14721, 9633, 13526, 4156, 11864, 6589, 7999, 111, 10675, 13594, 7000, 428, 7357, 5376, 6270, 6699, 12776, 6599, 5554, 9033, 9739, 2743, 2190, 7661, 9779, 10600, 5794, 9642, 6599, 12981, 9633, 2743, 10522, 2061, 2949, 6051, 9779, 5258, 5661, 6249, 5101, 3419, 5955, 9119, 1510, 9633, 11753, 2281, 1623, 8598, 6599, 6231, 9633, 3908, 6599, 6599, 9633, 13209, 7637, 12222, 386, 8495, 1553, 2560, 10045, 1031, 10496, 5661, 9841, 6599, 11139, 4108, 2209, 8536, 428, 9135, 5887, 8161, 9722, 2449, 9779, 2339, 6367, 3550, 10313, 12728, 14565, 9633, 4987, 9660, 9835, 10624, 12609, 9633, 9633, 13715, 10184, 2743, 1046, 3114, 8614, 6696, 10045, 10045, 14111, 9249, 3924, 8054, 8094, 8302, 4697, 8274, 7894, 1878, 2200, 10075, 9633, 5946, 6455, 12970, 9633, 13124, 6271, 6971, 8274, 1296, 8274, 9439, 13348, 10197, 13710, 9296, 6846, 5726, 2996, 6599, 6450, 5884, 6599, 8760, 11629, 7041, 2061, 9633, 6051, 9633, 6599, 11679, 6651, 2330, 9086]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0196078431372549\n",
      "0.0196078431372549\n"
     ]
    }
   ],
   "source": [
    "print(np.min(np.array(x)[0]))\n",
    "print(np.min(np.array(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
